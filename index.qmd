# CAPSTONE PROJECT

## Machine Learning and Predictive Analytics for Heart Disease Prediction

### Introduction

#### Problem Defination

Heart disease is still one of the worst public health crises worldwide. The World Health Organization (WHO) reports that cardiovascular diseases claim approximately 17.9 million lives each year, which is almost one-third of all deaths on the planet. Despite tremendous advances in medicine, detection of heart disease at its early stage continues to be an issue. Interpretation by doctors of medical data such as ECG readings forms the cornerstone of traditional diagnosis, cholesterol level, and patient history, which in certain instances can lead to inconsistent or delayed conclusions.

In the last couple of years, the emergence of data science and machine learning has opened up new avenues to solve this problem. Machine learning algorithms have provided promising results in identifying patterns in medical data that are not apparent to the human eye. Predictive analytics can be used to forecast the risk of a patient developing heart disease based on multiple indicators. This can help health professionals with early identification, preventive measures, and resource planning. The main problem addressed in this research is how to build and evaluate a reliable machine learning model that can predict heart disease from patient health data with high accuracy and interpretability#### What problem or gap in the existing literature does the paper aim to address?

This study aims to address the systematic evaluation of explainability techniques in predictive process analytics, which has been recognized as an open problem. While the literature is such that quite a number of papers exist that are concerned with the improving predictive models or proposing new XAI methods, no standard evaluation frameworks have been developed to evaluate the methods in terms of reliability, stability, and fidelity. This study develops functionally-grounded evaluation metrics for measuring stability and fidelity of explanations produced by some of the popular XAI methods, namely LIME and SHAP, as well as checks whether these methods deliver consistent and faithful explanations for predictive process models constructed using XGBoost, a widely employed machine learning algorithm known for its high classification accuracy.

#### Context and Background

Predictive analytics and machine learning are core areas of data science that use statistical and computational methods to model and predict data. Predictive modeling in healthcare helps to establish disease risk factors, evaluate treatment outcomes, and support medical decision-making. Research has looked at the use of algorithms such as Logistic Regression, Decision Trees, Random Forest, and Neural Networks for medical prediction. These models rely based on mathematical functions to model health indicator-disease co-occurrence relationships.

Logistic Regression, for example, uses a sigmoid function to model probability of binary outcome (disease or no disease), while Decision Trees and Random Forests provide rule-based approaches that divide the data into more manageable pieces. Healthcare machine learning also involves advanced notions like feature engineering, dimensionality reduction, and hyperparameter tuning, which improve the precision and effectiveness of prediction models. This project utilizes these principles to develop a data-intensive approach to predict heart disease risk using open-source medical data.

#### Objective and Goals

The main objective of the project is to develop a machine learning model to predict heart disease with high accuracy and interpretability. The study objectives are as follows: 1. To collect, preprocess, and analyze a publicly available heart disease dataset. 2. To apply various machine learning algorithms—Logistic Regression, Decision Tree, Random Forest, and Support Vector Machine (SVM)—and compare their performances. 3. To contrast models on the basis of statistical performance metrics such as accuracy, precision, recall, F1-score, and AUC. 4. To identify the most important health features for the prediction of heart disease. 5. To examine the practical implications of the results for the improvement of preventive healthcare practice.

By achieving the above goals, the study will demonstrate how data-driven modeling can assist healthcare professionals in making informed clinical choices and improving patient results.

#### Summary of Approach

This project follows a systematic machine learning pipeline right from data collection and preprocessing, to exploratory data analysis, feature selection, and model training. The research will use different classification algorithms and tune their parameters to provide the most predictive performance. The predictive performance of the models will be compared on key evaluation metrics. Visualizations will also be used to clarify model results and illustrate the impact of different features. The project combines statistical thinking, mathematical modeling, and computer science methodologies to create an integrated predictive framework for heart disease analysis.

### METHODS USED

#### Data Acquisition and Sources

The data set used in this project is Cleveland Heart Disease data set of the UCI Machine Learning Repository. It contains 303 patient records with 14 features such as age, sex, resting blood pressure, cholesterol, fasting blood sugar, maximum heart rate, and presence of heart disease. The target variable is indication of whether the patient has heart disease (1) or not (0).

Before using the data, preprocessing steps will include missing value management, numerical feature normalization, encoding categorical features (gender and chest pain type), and removal of outliers. Feature scaling methods like Min-Max normalization or StandardScaler will be used to place numerical variables in similar ranges to ensure stable model training. Data exploration through correlation matrices and visualization will be performed to gain insights into feature correlations.

#### Mathematical and Statistical Models

Several supervised learning algorithms will be used to model the data. These include:

-   **Logistic Regression:** A statistical model that predicts the probability of heart disease occurrence based on a linear combination of input features.

$$
P(Y=1 \mid X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_n X_n)}}
$$

-   **Decision Tree Classifier:** A model that splits data into branches using decision rules, allowing for simple interpretation.

$$
G(S, A) = Entropy(S) - \sum_{v \in Values(A)} \frac{|S_v|}{|S|} \, Entropy(S_v)
$$

-   **Random Forest:** An ensemble model that combines multiple decision trees to improve accuracy and reduce overfitting.

$$
\hat{y} = \text{majority\_vote}\left( h_1(x), h_2(x), \dots, h_k(x) \right)
$$

-   **Support Vector Machine (SVM):** A classification model that finds an optimal hyperplane separating patients with and without heart disease.

$$
\min_{w, b} \ \frac{1}{2} \|w\|^2 \quad \text{subject to} \quad y_i (w^T x_i + b) \ge 1, \ \forall i
$$

Each of these algorithms uses mathematical and statistical principles for pattern recognition. Logistic regression uses maximum likelihood estimation; Decision Trees rely on information gain and Gini impurity; SVMs use kernel functions to map nonlinear data. Model selection and hyperparameter tuning will be done through grid search or cross-validation techniques.

#### ![](images/WhatsApp%20Image%202025-10-08%20at%2019.57.13.jpeg)Experimental Design or Analytical Procedures

The workflow for this project includes several analytical stages. The dataset will be divided into training and testing subsets using an 80/20 ratio. Model training will occur on the training subset, and model testing will evaluate generalization on unseen data. Cross-validation (k-fold) will be performed to ensure that model results are not due to random splits. The models’ performance will be compared using metrics such as Accuracy, Precision, Recall, F1-score, and Area Under the ROC Curve (AUC). Confusion matrices and ROC curves will be generated for visualization.\

Feature importance will be analyzed to identify which attributes (such as cholesterol level, blood pressure, or age) contribute most significantly to predicting heart disease. Sensitivity analysis will also be performed to evaluate how small changes in inputs affect model predictions, ensuring robustness.\
\

| Variable | Description | Type | Example Value |
|----|----|----|----|
| **age** | Age of the patient (in years) | Numeric | 54 |
| **sex** | Gender of the patient (1 = male, 0 = female) | Categorical | 1 |
| **cp** | Chest pain type (0–3, indicating severity/type) | Categorical | 2 |
| **trestbps** | Resting blood pressure (mm Hg) | Numeric | 130 |
| **chol** | Serum cholesterol level (mg/dl) | Numeric | 246 |
| **thalach** | Maximum heart rate achieved | Numeric | 150 |

#### Software and Tools

All computations will be performed using Python. The libraries and tools used include:

-   pandas and NumPy for data preprocessing and analysis,
-   scikit-learn for machine learning algorithms and model evaluation,
-   matplotlib and seaborn for visualization, and
-   Jupyter Notebook as the development environment.

Data preprocessing, training, and evaluation will be done locally using standard computing resources. The software selection ensures flexibility, reproducibility, and transparency of the research workflow.

#### Ethical Considerations

As the dataset is publicly available and fully anonymized, there are no direct ethical risks associated with privacy or patient confidentiality. However, care will be taken to ensure that the results are interpreted responsibly. The predictive models developed in this study are for educational and research purposes only and should not be used for real clinical diagnosis without professional validation. Additionally, ethical use of data includes acknowledging sources, avoiding bias in model design, and ensuring transparency in reporting results.

::: {align="right"}
Madhu Kiran Reddy Koduru
:::
